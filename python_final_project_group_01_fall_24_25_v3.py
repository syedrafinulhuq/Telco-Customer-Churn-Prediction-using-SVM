# -*- coding: utf-8 -*-
"""python_final_project_group_01_fall_24_25_V3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17LVXKq2csfXt9pfBP0H2mmuoL1-h4Aly

```
Student-1
Name: A. M. Rafinul Huq
ID: 21 -45668-3
```

```
Student-2
Name: YEASIR AHNAF ASIF
ID: 20-42815-1
```

*   Mount your google drive.
"""

from google.colab import drive
drive.mount('/content/drive')

"""

*   Import all necessary libraries.

"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import GridSearchCV, cross_val_score

"""•	**Task 1:** Read/Load the dataset file in your program. Use Pandas library to complete this task."""

path = '/content/drive/My Drive/telco_churn.csv'
data = pd.read_csv(path)
print(data.head())

"""•	**Task 2:** Apply appropriate data cleaning techniques to the dataset. In this step, replace bad data using proper methods and do not delete any record except duplicate records. Use Pandas library to complete this task."""

data.info()


data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data['TotalCharges'].fillna(data['TotalCharges'].median(), inplace=True)


data.drop_duplicates(inplace=True)

data.info()

"""•	**Task 3:** Draw graphs to analyze the frequency distributions of the features. Use Matplotlib library to complete this task. Draw all the plots in a single figure so that all plots can be seen in one diagram (use subplot() function)."""

categorical_features = ['gender', 'Partner', 'Dependents', 'PhoneService', 'Churn']
numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']

fig, axes = plt.subplots(2, 5, figsize=(25, 10))


for i, feature in enumerate(categorical_features):
    ax = axes[0, i]
    data[feature].value_counts().plot(kind='bar', ax=ax, title=feature)
    ax.set_ylabel("Count")


for i, feature in enumerate(numerical_features):
    ax = axes[1, i]
    data[feature].plot(kind='hist', bins=20, ax=ax, title=feature)
    ax.set_xlabel("Value")

for j in range(len(categorical_features), 5):
    fig.delaxes(axes[0, j])
for j in range(len(numerical_features), 5):
    fig.delaxes(axes[1, j])

plt.tight_layout()
plt.show()

"""•	**Task 4:** Perform scaling to the features of the dataset. Remember that you will need to apply data conversion before performing scaling whenever necessary."""

label_encoders = {}
categorical_cols = data.select_dtypes(include=['object']).columns

for col in categorical_cols:
    if col != 'customerID':
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le


scaler = StandardScaler()
numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])


data.head()

"""•	**Task 5:** Split your data into two parts: Training dataset and Testing dataset. You must use the function train_test_split() to complete this task and use value 3241 as the value of the random_state parameter of this function."""

X = data.drop(columns=['customerID', 'Churn'])
y = data['Churn']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3241)


X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""•	**Task 6:** Apply Support Vector Machine (SVM) Classifier to the dataset. Build (train) your prediction model in this step."""

svm_classifier = SVC(kernel='linear', random_state=3241)

svm_classifier.fit(X_train, y_train)


y_train_pred = svm_classifier.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
print("Training Accuracy (Default Parameters):", train_accuracy)


param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': [0.1, 1, 10]
}

grid_search = GridSearchCV(SVC(random_state=3241), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)


best_params = grid_search.best_params_
print("Best Parameters from GridSearch:", best_params)

best_svm_classifier = grid_search.best_estimator_
best_svm_classifier.fit(X_train, y_train)

"""•	**Task 7:** Calculate the confusion matrix for your model. Interpret it in detail in the report."""

y_test_pred = best_svm_classifier.predict(X_test)


conf_matrix = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=best_svm_classifier.classes_)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix for Best Model")
plt.show()


print("Classification Report:\n", classification_report(y_test, y_test_pred))

"""•	**Taks 8:** Calculate the train and test accuracy of your model and compare them."""

train_accuracy_tuned = accuracy_score(y_train, best_svm_classifier.predict(X_train))
test_accuracy_tuned = accuracy_score(y_test, y_test_pred)

print("Training Accuracy with Tuned Parameters:", train_accuracy_tuned)
print("Testing Accuracy with Tuned Parameters:", test_accuracy_tuned)


accuracy_difference = abs(train_accuracy_tuned - test_accuracy_tuned)
print("Accuracy Difference:", accuracy_difference)